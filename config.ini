[extractor]
output_stack_size = 8192
model_output_folder = /scratch/gautschi/patil185/artifacts/extracted/models
default_extractor_class = HuggingFacePipelineExtractor
extractor_registry = {
    "Fashion-Mnist-SigLIP2": "SiglipExtractor",
    "Mnist-Digits-SigLIP2": "SiglipExtractor",
    "resnet34_svhn": "DetectorBasedExtractor",
    "densenet121_svhn": "DetectorBasedExtractor",
    "vgg16_bn_svhn": "DetectorBasedExtractor",
    "resnet34_cifar100": "DetectorBasedExtractor",
    "resnet18_cifar100": "DetectorBasedExtractor",
    }

[autoencoder]
; A single linear layer is often enough for very small datasets; add more depth for complex parameter spaces.
encoder_input_size = 8192
encoder_output_size = 512
decoder_output_size = 8192
encoder_hidden_layers = [1024]
decoder_hidden_layers = [1024]
use_activation = true
dropout = 0.05
activation = gelu

[train_model_autoencoder]
batch_size = 128
num_epochs = 3000
learning_rate = 0.001
shuffle = true
log_every_n_epochs = 5
extracted_models_dir = /scratch/gautschi/patil185/artifacts/extracted/models
model_save_directory = /scratch/gautschi/patil185/artifacts/models/model_autoencoder
progress_description = ModelAutoEncoder
weight_decay = 0.0001
beta1 = 0.9
beta2 = 0.999
gradient_clip_norm = 1.0
early_stopping_patience = 40
early_stopping_min_delta = 0.0001
scheduler_factor = 0.5
scheduler_patience = 15
scheduler_min_lr = 0.00001
normalize_inputs = true
code_l1_penalty = 0.001
reconstruction_loss = smooth_l1
smooth_l1_beta = 0.5

[imagenet_dataset]
dataset_name = ILSVRC/imagenet-1k
split = train
cache_dir = /scratch/gautschi/patil185/artifacts/data/imagenet
image_size = 224
resize_shorter_side = 256
drop_last = true
shuffle = true
seed = 42
num_workers = 4
pin_memory = true
persistent_workers = true
prefetch_factor = 2

[clip_evaluation]
model_name = openai/clip-vit-base-patch32
dataset_split = validation
device = cuda
precision = fp16
normalize_features = true
num_batches = 1
output_directory = /scratch/gautschi/patil185/artifacts/extracted/imagenet_clip
split_sample_counts = {"train": 131072, "validation": 8192, "test": 65536}
batches_per_shard = 512
pad_to_full_shard = true
