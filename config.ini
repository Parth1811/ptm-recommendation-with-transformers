[extractor]
output_stack_size = 8192
model_output_folder = artifacts/extracted/models-test
default_extractor_class = HuggingFacePipelineExtractor
extractor_registry = {
    "Fashion-Mnist-SigLIP2": "SiglipExtractor",
    "Mnist-Digits-SigLIP2": "SiglipExtractor",
    "resnet34_svhn": "DetectorBasedExtractor",
    "densenet121_svhn": "DetectorBasedExtractor",
    "vgg16_bn_svhn": "DetectorBasedExtractor",
    "resnet34_cifar100": "DetectorBasedExtractor",
    "resnet18_cifar100": "DetectorBasedExtractor",
    }

[autoencoder]
; A single linear layer is often enough for very small datasets; add more depth for complex parameter spaces.
encoder_input_size = 8192
encoder_output_size = 512
decoder_output_size = 8192
encoder_hidden_layers = [1024]
decoder_hidden_layers = [1024]
use_activation = true
dropout = 0.05
activation = gelu

[train_model_autoencoder]
batch_size = 16
num_epochs = 1500
num_workers = 64
device = cuda
learning_rate = 0.001
shuffle = true
log_every_n_epochs = 5
save_checkpoint_every_n_epochs = 100
validate_every_n_epochs = 2
extracted_models_dir = artifacts/extracted/models
model_save_directory = artifacts/models/model_autoencoder
progress_description = ModelAutoEncoder
weight_decay = 0.0001
beta1 = 0.9
beta2 = 0.999
gradient_clip_norm = 1.0
early_stopping_patience = 200
early_stopping_min_delta = 0.00001
scheduler_factor = 0.5
scheduler_patience = 10
scheduler_min_lr = 0.00001
normalize_inputs = true
code_l1_penalty = 0.001
reconstruction_loss = smooth_l1
smooth_l1_beta = 0.5

[dataset_defaults]
cache_dir = artifacts/data
split_ratios = [0.8, 0.1, 0.1]
seed = 42
drop_last = false
shuffle = false
num_workers = 4
pin_memory = true
persistent_workers = true
prefetch_factor = 2
image_column = image
label_column = label
balance_classes = true
max_samples_train = 131072
max_samples_validation = 8192
max_samples_test = 65536

[clip_evaluation]
model_name = openai/clip-vit-base-patch32
device = cuda
precision = fp16
normalize_features = true
output_directory = artifacts/extracted/datasets
batches_per_shard = 16
cache_directory_override =
pad_to_full_shard = true
; dataset_names = ["MNIST", "SVHN", "USPS Digits", "Fashion-MNIST", "Deepfashion Inshop", "Fashion Product", "ImageNet 1k", "CIFAR-10", "CIFAR-100", "Caltech-101", "HAM10000", "LIDC-IDRI", "BCW"]
dataset_names = ["USPS Digits"]
limit_batches_per_split = None
extra_load_kwargs = {}

[dataset_loader]
default_loader_class = GenericBalancedDataLoader
loader_registry = {
    "MNIST": {"dataset_name": "ylecun/mnist"},
    "SVHN": {"dataset_name": "ufldl-stanford/svhn", "dataset_config": "cropped_digits"},
    "USPS Digits": {"dataset_name": "Voxel51/USPS"},
    "Fashion-MNIST": {"dataset_name": "zalando-datasets/fashion_mnist"},
    "Deepfashion Inshop": {"dataset_name": "Marqo/deepfashion-inshop", "label_column": "category2"},
    "Fashion Product": {"dataset_name": "ashraq/fashion-product-images-small", "label_column": "usage"},
    "ImageNet 1k": {"dataset_name": "ILSVRC/imagenet-1k"},
    "CIFAR-10": {"dataset_name": "uoft-cs/cifar10", "image_column": "img", "label_column": "label"},
    "CIFAR-100": {"dataset_name": "uoft-cs/cifar100", "image_column": "img", "label_column": "fine_label"},
    "Caltech-101": {
        "dataset_name": "bitmind/caltech-101",
        "label_from_filename": {"source_column": "filename", "target_column": "label", "separator": "/", "index": 0, "strip_extension": True, "lowercase": True},
        "label_column": "label"
    },
    "HAM10000": {"dataset_name": "Nagabu/HAM10000"},
    "LIDC-IDRI": {"dataset_name": "jmanuelc87/lidc-idri-segmentation"},
    "BCW": {"dataset_name": "scikit-learn/breast-cancer-wisconsin"}
    }

[autoencoder_evaluation]
weights_path = artifacts/models/model_autoencoder/autoencoder_weights.loss_0.008648.20251103_085137.pt
parameter_root = artifacts/extracted/models-test
output_directory = artifacts/extracted/model_test_embeddings
batch_size = 256
device = cuda
normalize_inputs = true
flatten = true
input_dtype = float32
file_substring = ""
save_dtype = float32

[model_embedding_loader]
root_dir = artifacts/extracted/model_embeddings
embedding_key = embedding
batch_size = 16
max_models = 16
shuffle = true
num_workers = 0
pin_memory = true

[dataset_token_loader]
root_dir = artifacts/extracted/datasets
dataset_names = None
shard_glob = "*.npz"
batch_size = 16
shuffle = true
include_class_metadata = true
num_workers = 0
pin_memory = true

[ranking_cross_attention_transformer]
; Cross-attention transformer for model-dataset ranking using torch.nn.Transformer
; Model architecture parameters
d_model = 512
nhead = 8
num_encoder_layers = 6
num_decoder_layers = 6
dim_feedforward = 2048
dropout = 0.1
num_models = 16

[transformer_trainer]
; Training hyperparameters
batch_size = 1
num_epochs = 500
num_workers = 64
device = cuda
learning_rate = 0.0001
weight_decay = 0.001
shuffle = true
gradient_clip_norm = 1.0
early_stopping_patience = 500
early_stopping_min_delta = 0.00001
scheduler_factor = 0.5
scheduler_patience = 15
scheduler_min_lr = 0.000001
validate_every_n_epochs = 1
log_every_n_epochs = 1
save_checkpoint_every_n_epochs = 50
ranking_loss_weight = 0.1
smooth_l1_weight = 0.1
model_save_directory = artifacts/models/transformer
progress_description = TransformerTrainer
load_from_checkpoint = false
checkpoint_path = artifacts/models/transformer/checkpoint_CrossAttentionTransformer_epoch_75_20251112_162856.pt
only_load_model_weights = false

[test_transformer]
; Test configuration for TransformerTrainer evaluation on test set
checkpoint_path = artifacts/models/transformer/CrossAttentionTransformer_best_20251112_162856.pt
device = cuda
output_directory = artifacts/test_results
output_filename = transformer_blind_test_results.csv