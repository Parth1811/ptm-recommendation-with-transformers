\section{Motivation and Research Problem}

Modern machine learning increasingly relies on reusing pre-trained models (PTMs), yet practitioners face three fundamental challenges when selecting an appropriate model for a downstream task.

First, the scale and diversity of today's model repositories make discovery itself difficult~\cite{wolf2020transformers,hfcommunity2024dataset,jiang2024peatmoss,ding2024ptmtorrent}. With millions of PTMs differing in architecture, size, modality, training objectives, and data sources, practitioners lack systematic guidance for identifying which models are even plausible candidates. The consequence is an overwhelming and largely unstructured search space.

Second, evaluating candidate PTMs through fine-tuning is computationally prohibitive~\cite{schwartz2020green,wu2022sustainable}. Even tuning a single large model can require hours of GPU time, careful hyperparameter design, and repeated experimentation. Extending this process to tens or hundreds of models—often necessary for empirical selection—quickly becomes infeasible in practice. As a result, developers either overspend resources or settle for suboptimal models.

Third, existing PTM selection practices and automated recommenders are misaligned with deployment constraints~\cite{zhang2023modelspider,meng2023foundation,liu2025ptmpicker}. They prioritize accuracy-based metrics while largely ignoring the hardware budgets under which the model must operate—memory capacity, compute throughput, latency requirements, and energy limitations~\cite{schwartz2020green}. This omission is especially problematic in heterogeneous environments such as IoT deployments, where devices differ drastically in capability and many applications impose strict real-time or near-real-time constraints. Moreover, current recommenders often rely on fixed model zoos or shallow metadata and therefore cannot generalize to newly released PTMs or capture deeper dataset–model–hardware relationships~\cite{zhang2023modelspider}.

These limitations highlight the absence of a unified, learning-based framework capable of predicting dataset–model–hardware compatibility before fine-tuning. This thesis investigates whether such a framework can be achieved through a data-driven approach that learns from large-scale PTM usage. The central research question is whether a deep learning model can reliably estimate PTM suitability in a way that reduces selection cost, improves generalizability, and enables more efficient model development across diverse deployment environments.
