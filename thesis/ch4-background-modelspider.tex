\subsection{Model Spider}
\label{subsec:model-spider}

Model Spider~\cite{zhang2023modelspider} represents a significant advance in pre-trained model recommendation, introducing the first transformer-based learning approach to predict model-dataset compatibility without requiring exhaustive fine-tuning experiments. Unlike prior heuristic methods that rely on forward passes through candidate models or hand-crafted transferability metrics, Model Spider employs a data-driven framework that learns to rank models efficiently by modeling the relationship between dataset characteristics and model representations through self-attention mechanisms.

The architecture operates on two distinct input modalities: \textbf{dataset tokens} and \textbf{model tokens}. Dataset tokens $\mu(\mathcal{T}_d)$ are derived by sampling representative images from each class in the target dataset and processing them through a feature extractor $\Psi_d$ (typically a pre-trained vision model such as ResNet or Vision Transformer), yielding a fixed-dimensional representation that captures the statistical and semantic properties of the dataset. Model tokens $\{\boldsymbol{\theta}_m\}_{m=1}^{M}$ are learnable embedding vectors, where each vector $\boldsymbol{\theta}_m \in \mathbb{R}^{d}$ corresponds to a specific pre-trained model in the model zoo. These dataset and model tokens are concatenated and fed into a single-layer multi-head self-attention block, which computes interactions between all tokens. The self-attention mechanism allows the network to learn which dataset features correlate with which model representations, ultimately producing a probability distribution over the $M$ candidate models via a classification head. Models are then ranked according to their predicted suitability scores for the given dataset.

To further refine predictions, Model Spider incorporates an optional \textbf{enrichment layer} that performs actual forward passes through the top-$k$ ranked models on a small validation subset of the target dataset. This enrichment step provides empirical performance signals that can improve ranking accuracy, particularly for distinguishing among similarly-scored candidates. However, this component introduces computational overhead proportional to the number of enriched models and the size of the validation set, partially undermining the efficiency gains of the learning-based approach. Consequently, enrichment is typically applied sparingly, limited to a small subset of top-ranked candidates (e.g., $k=5$ to $10$) to balance prediction quality with computational cost.

A critical limitation of Model Spider lies in its representation of candidate models through learnable token embeddings. Each model token $\boldsymbol{\theta}_m$ is initialized randomly and optimized during training to capture the characteristics of a specific model in the fixed model zoo. This design inherently ties the learned embeddings to the identity of models seen during trainingâ€”for instance, the fifth token might learn to represent ResNet-50, while the tenth token represents EfficientNet-B3. Consequently, \textbf{Model Spider cannot generalize to unseen models} that were not present in the training set. When a new pre-trained model becomes available, there exists no mechanism to translate its architecture or learned weights into an appropriate token embedding without retraining the entire system. This restriction limits the framework's applicability in the rapidly evolving landscape of pre-trained models, where new architectures and training paradigms emerge continuously. This generalization challenge motivates the development of model encoding strategies that derive embeddings directly from model parameters rather than relying on learned proxy representations, enabling recommendation systems to accommodate newly released models without requiring retraining.
