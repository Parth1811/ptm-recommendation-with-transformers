% ============================================================================
% TRANSFORMER ARCHITECTURE - SECTION 2.5 REFERENCES
% ============================================================================

% Transformer Surveys and Tutorials
@article{lin2022survey,
  title={A Survey of Transformers},
  author={Lin, Tianyang and Wang, Yuxin and Liu, Xiangyang and Qiu, Xipeng},
  journal={AI Open},
  volume={3},
  pages={111--132},
  year={2022},
  publisher={Elsevier},
  doi={10.1016/j.aiopen.2022.10.001}
}

@article{khan2022transformers,
  title={Transformers in Vision: {A} Survey},
  author={Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
  journal={ACM Computing Surveys},
  volume={54},
  number={10s},
  pages={1--41},
  year={2022},
  publisher={ACM},
  doi={10.1145/3505244}
}

@book{tunstall2022natural,
  title={Natural Language Processing with Transformers: {B}uilding Language Applications with {H}ugging {F}ace},
  author={Tunstall, Lewis and von Werra, Leandro and Wolf, Thomas},
  year={2022},
  publisher={O'Reilly Media},
  isbn={978-1-098-10324-3}
}

% Architectural Variants and Formal Analysis
@inproceedings{phuong2022formal,
  title={Formal Algorithms for Transformers},
  author={Phuong, Mary and Hutter, Marcus},
  booktitle={arXiv preprint arXiv:2207.09238},
  year={2022},
  eprint={2207.09238},
  archivePrefix={arXiv}
}

@article{tay2022efficient,
  title={Efficient Transformers: {A} Survey},
  author={Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  journal={ACM Computing Surveys},
  volume={55},
  number={6},
  pages={1--28},
  year={2022},
  publisher={ACM},
  doi={10.1145/3530811}
}

% Large-Scale Transformer Training
@inproceedings{shoeybi2019megatron,
  title={{Megatron-LM}: {T}raining Multi-Billion Parameter Language Models Using Model Parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  booktitle={arXiv preprint arXiv:1909.08053},
  year={2019},
  eprint={1909.08053},
  archivePrefix={arXiv}
}
