\subsection{Baseline Comparisons}
\label{subsec:baseline-comparisons}

To establish competitive baselines, we utilize performance values reported in the Model Spider paper~\cite{zhang2023modelspider}, which includes evaluations of both the Model Spider approach itself and several heuristic transferability metrics (NCE, LEEP, LogME, among others) on standard benchmark datasets. This approach enables direct comparison against state-of-the-art methods while conserving computational resources that would otherwise be required for exhaustive re-implementation and re-evaluation of multiple baseline systems.

Two methodological differences between our evaluation and the Model Spider experiments should be acknowledged. First, our model zoo comprises 31 pre-trained models compared to Model Spider's 10 models; the Model Spider paper demonstrates that recommendation accuracy tends to decrease as model zoo size increases, potentially making our task more challenging and comparisons favorable to the baselines. Second, we extract dataset tokens using a single representative image per class, whereas Model Spider employs 10 images per class for token formation. Both differences represent more constrained conditions for Cross-Select relative to the baseline evaluations, ensuring that any observed performance advantages reflect genuine improvements rather than experimental artifacts favoring our approach.
