\section{Pre-Trained Models}
\label{sec:pretrained-models}

Pre-trained models (PTMs) are neural networks trained on large-scale datasets whose learned parameters are made publicly available for reuse on downstream tasks, thereby amortizing the substantial computational cost of training across multiple applications~\cite{devlin2019bert,he2016deep,brown2020language}. This paradigm has become ubiquitous across domains: in natural language processing, models such as BERT~\cite{devlin2019bert}, GPT~\cite{radford2019language}, and T5~\cite{raffel2020exploring} pre-trained on massive text corpora enable state-of-the-art performance on diverse tasks through fine-tuning; in computer vision, architectures like ResNet~\cite{he2016deep} and Vision Transformers~\cite{dosovitskiy2021image} pre-trained on ImageNet~\cite{deng2009imagenet} provide robust feature extractors for image recognition; and in multimodal learning, models such as CLIP~\cite{radford2021learning} bridge vision and language. The proliferation of PTM repositories including Hugging Face Hub~\cite{wolf2020transformers}, TensorFlow Hub~\cite{tensorflow2015whitepaper}, and PyTorch Hub~\cite{paszke2019pytorch} has democratized access to thousands of pre-trained models, significantly reducing barriers to entry for practitioners with limited computational resources or labeled data. However, this abundance introduces a critical challenge: determining which pre-trained model is most suitable for a given target task among the vast and growing collection of available options, motivating the need for automated model recommendation systems.
